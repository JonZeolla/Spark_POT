{"nbformat_minor": 0, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Lab 3 - Spark MLlib\n\n##### \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P, improves with experience E\"\n-Tom M. Mitchell\n\n#### Machine Learning - the science of getting computers to act without being explicitly programmed\n\nMLlib is Spark\u2019s machine learning (ML) library. Its goal is to make practical machine learning scalable and easy. It consists of common learning algorithms and utilities, including classification, regression, clustering, collaborative filtering (this example!), dimensionality reduction, as well as lower-level optimization primitives and higher-level pipeline APIs.\n\nIt divides into two packages:\n- spark.mllib contains the original API built on top of RDDs.\n- spark.ml provides higher-level API built on top of DataFrames for constructing ML pipelines.\n\n\nUsing spark.ml is recommended because with DataFrames the API is more versatile and flexible. But we will keep supporting spark.mllib along with the development of spark.ml. Users should be comfortable using spark.mllib features and expect more features coming.\n\nhttp://spark.apache.org/docs/latest/mllib-guide.html\n\n## Online Purchase Recommendations\n\nLearn how to create a recommendation engine using the Alternating Least Squares algorithm in Spark's machine learning library\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/ALS.png' width=\"70%\" height=\"70%\"></img>\n\n\nWorkflow:\n\n<img src='https://raw.githubusercontent.com/bradenrc/Spark_POT/master/Modules/MachineLearning/Collaborative%20Filtering/als_flow.png' width=\"70%\" height=\"70%\"></img>\n\n\n### The data\n\nThis is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.  The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n\nhttp://archive.ics.uci.edu/ml/datasets/Online+Retail\n\n<img src='https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/FullFile.png' width=\"80%\" height=\"80%\"></img>"}, {"cell_type": "markdown", "metadata": {}, "source": "## Download The Data: \n- The OnlineRetail.csv.gz csv file will be used to create the RDD(loadRetailData) in the following paragraph"}, {"execution_count": 1, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "--2016-06-25 16:22:30--  https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7483128 (7.1M) [application/octet-stream]\nLast-modified header missing -- time-stamps turned off.\n--2016-06-25 16:22:30--  https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz\nReusing existing connection to raw.githubusercontent.com:443.\nHTTP request sent, awaiting response... 200 OK\nLength: 7483128 (7.1M) [application/octet-stream]\nSaving to: 'OnlineRetail.csv.gz'\n\n100%[======================================>] 7,483,128   --.-K/s   in 0.1s    \n\n2016-06-25 16:22:31 (72.0 MB/s) - 'OnlineRetail.csv.gz' saved [7483128/7483128]\n\n"}], "source": "!wget https://raw.githubusercontent.com/rosswlewis/RecommendationPoT/master/OnlineRetail.csv.gz -N", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare and Shape Data:  \"80% of a Data Scientists Job\"\n- Data preparation accounts for about 80% of the work of data scientists \n  - Data scientists spend 60% of their time on cleaning and organizing data. Collecting data sets comes second at 19% of their time, meaning data scientists spend around 80% of their time on preparing and managing data for analysis.\n\n - Using a combination of Machine Learning, advanced analytics, and statistical methods to prepare data for use in data modeling.\n\n<img src= \"http://blogs-images.forbes.com/gilpress/files/2016/03/Time-1200x511.jpg\"width=\"50%\" height=\"50%\"></img>\n**Data Citation - Forbes: Gil Press, Contributor  **\nCleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says\nhttp://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#1137675d7f75"}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare and Shape Data:\n- Put the csv into an RDD (at first, each row in the RDD is a string which correlates to a line in the csv)"}, {"execution_count": 2, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "InvoiceNo,StockCode,Description,Quantity,InvoiceDate,UnitPrice,CustomerID,Country\n536365,85123A,WHITE HANGING HEART T-LIGHT HOLDER,6,12/1/10 8:26,2.55,17850,United Kingdom\n536365,71053,WHITE METAL LANTERN,6,12/1/10 8:26,3.39,17850,United Kingdom\n536365,84406B,CREAM CUPID HEARTS COAT HANGER,8,12/1/10 8:26,2.75,17850,United Kingdom\n536365,84029G,KNITTED UNION FLAG HOT WATER BOTTLE,6,12/1/10 8:26,3.39,17850,United Kingdom\n"}], "source": "loadRetailData = sc.textFile(\"OnlineRetail.csv.gz\")\n\nfor row in loadRetailData.take(5):\n    print row", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare and Shape Data:  \n- First we will pull the CSV data into a format that is usable by\n - The csv file had a Header record\n   - Remove the header from the RDD \n - Split the string in each row by comma\n - Print out sample data from RDD(loadRetailData)"}, {"execution_count": 3, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "[u'536365', u'85123A', u'WHITE HANGING HEART T-LIGHT HOLDER', u'6', u'12/1/10 8:26', u'2.55', u'17850', u'United Kingdom']\n[u'536365', u'71053', u'WHITE METAL LANTERN', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']\n[u'536365', u'84406B', u'CREAM CUPID HEARTS COAT HANGER', u'8', u'12/1/10 8:26', u'2.75', u'17850', u'United Kingdom']\n[u'536365', u'84029G', u'KNITTED UNION FLAG HOT WATER BOTTLE', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']\n[u'536365', u'84029E', u'RED WOOLLY HOTTIE WHITE HEART.', u'6', u'12/1/10 8:26', u'3.39', u'17850', u'United Kingdom']\n"}], "source": "#remove the first row from the RDD. The first row contains header info \nheader = loadRetailData.first()\nloadRetailData = loadRetailData.filter(lambda line: line != header).\\\n                            map(lambda l: l.split(\",\"))\n\n\nfor row in loadRetailData.take(5):\n    print row", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare and Shape Data:\n\n- import re\n  - This module provides regular expression matching operations similar to those found in Perl.<br>\n- Only keep rows that have\n  - purchase quantity of greater than 0, \n  - customerID not equal to 0, \n  - a non blank stock code after removing non-numeric characters.\n\n\n**Note:  The original file at UCI's Machine Learning Repository has commas in the product description.  Those have been removed to expediate the lab.**\n"}, {"execution_count": 4, "cell_type": "code", "outputs": [], "source": "# clean up data. Keep only rows that have a value in quanity which is offset 3. \nimport re\nloadRetailData = loadRetailData.filter(lambda l: int(l[3]) > 0\\\n                                and len(re.sub(\"\\D\", \"\", l[1])) != 0 \\\n                                and len(l[6]) != 0)", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare and Shape Data:\n\n  - Map each line to a row \n  - Create a Dataframe(retailDF)\n  - Print Schema(printSchema)\n  - Create a Temporary Table in the SQLContext (retailPurchases)\n  - Select from temp table (retailPurchases) using pandas library"}, {"execution_count": 5, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- country: string (nullable = true)\n |-- custId: long (nullable = true)\n |-- description: string (nullable = true)\n |-- inv: long (nullable = true)\n |-- invDate: string (nullable = true)\n |-- price: double (nullable = true)\n |-- quant: long (nullable = true)\n |-- stockCode: long (nullable = true)\n\nNone\n"}, {"data": {"text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>country</th>\n      <th>custId</th>\n      <th>description</th>\n      <th>inv</th>\n      <th>invDate</th>\n      <th>price</th>\n      <th>quant</th>\n      <th>stockCode</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>United Kingdom</td>\n      <td>17850</td>\n      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n      <td>536365</td>\n      <td>12/1/10 8:26</td>\n      <td>2.55</td>\n      <td>6</td>\n      <td>85123</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>United Kingdom</td>\n      <td>17850</td>\n      <td>WHITE METAL LANTERN</td>\n      <td>536365</td>\n      <td>12/1/10 8:26</td>\n      <td>3.39</td>\n      <td>6</td>\n      <td>71053</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "          country  custId                         description     inv  \\\n0  United Kingdom   17850  WHITE HANGING HEART T-LIGHT HOLDER  536365   \n1  United Kingdom   17850                 WHITE METAL LANTERN  536365   \n\n        invDate  price  quant  stockCode  \n0  12/1/10 8:26   2.55      6      85123  \n1  12/1/10 8:26   3.39      6      71053  "}, "execution_count": 5, "output_type": "execute_result", "metadata": {}}], "source": "# Add column names to the RDD and create Dataframe from RDD which now has column names\nfrom pyspark.sql import SQLContext, Row\nsqlContext = SQLContext(sc)\n\n# Convert each line to a Row adding column names.\n# Create a new RDD, now with column names\n\nloadRetailData = loadRetailData.map(lambda l: Row(inv=int(l[0]),\\\n                                    stockCode=int(re.sub(\"\\D\", \"\", l[1])),\\\n                                    description=l[2],\\\n                                    quant=int(l[3]),\\\n                                    invDate=l[4],\\\n                                    price=float(l[5]),\\\n                                    custId=int(l[6]),\\\n                                    country=l[7]))\n\n# Infer the schema, and register the DataFrame as a table.\n# Create a new DataFrame \nretailDf = sqlContext.createDataFrame(loadRetailData)\nprint retailDf.printSchema()\n\nretailDf.registerTempTable(\"retailPurchases\")\nsqlContext.sql(\"SELECT * FROM retailPurchases limit 2\").toPandas()", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Filter To Needed Columns:\n\n- Select only data that will be used to create new Dataframe(retailDf)\n  - Select only the data we need (custId, stockCode, and rank)\n  - Create DataFrame(retailDf)\n  - Creating a Temporary Table in the SQLContext (retailDf)"}, {"execution_count": 6, "cell_type": "code", "outputs": [], "source": "# select columns custId, stockCode and add column for purch with a value of 1 to indicate a purchase. \n# re-create Dataframe(retailDf) frome SQL query \n\nquery = \"\"\"\nSELECT \n    custId, stockCode, 1 as purch\nFROM \n    retailPurchases \ngroup \n    by custId, stockCode\"\"\"\nretailDf = sqlContext.sql(query)\nretailDf.registerTempTable(\"retailDf\")", "metadata": {"collapsed": false}}, {"execution_count": 7, "cell_type": "code", "outputs": [{"data": {"text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>custId</th>\n      <th>stockCode</th>\n      <th>purch</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>12838</td>\n      <td>22941</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17968</td>\n      <td>22731</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>16210</td>\n      <td>20977</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>17897</td>\n      <td>84558</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>16552</td>\n      <td>85123</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>17905</td>\n      <td>21662</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>13468</td>\n      <td>21231</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>16274</td>\n      <td>21809</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>13090</td>\n      <td>22617</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>16186</td>\n      <td>22865</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   custId  stockCode  purch\n0   12838      22941      1\n1   17968      22731      1\n2   16210      20977      1\n3   17897      84558      1\n4   16552      85123      1\n5   17905      21662      1\n6   13468      21231      1\n7   16274      21809      1\n8   13090      22617      1\n9   16186      22865      1"}, "execution_count": 7, "output_type": "execute_result", "metadata": {}}], "source": "# Print sample of the data from temp table(retailDf) created from Dataframe(retailDf)\nsqlContext.sql(\"select * from retailDf limit 10\").toPandas()", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Split into 3 Dataframes:\n\n  - Randomly split the data into 3 Dataframes, to be used for testing and training the models to be created:<br>\n   - testing set (10% of the data)<br>\n   - cross validation set (10% of the data)<br>\n   - training set (80% of the data)<br>"}, {"execution_count": 8, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "trainDf count:  208116  example: \nRow(custId=17968, stockCode=22731, purch=1)\nRow(custId=17897, stockCode=84558, purch=1)\n\ncvDf count:  25869  example: \nRow(custId=12838, stockCode=22941, purch=1)\nRow(custId=13468, stockCode=21231, purch=1)\n\ntestDf count:  26127  example: \nRow(custId=16210, stockCode=20977, purch=1)\nRow(custId=13090, stockCode=22617, purch=1)\n\n"}], "source": "# split the orgional Dataframe(retailDf) into 3 Dataframes. This will provide the ability to test different models \n# testDf = %10, cvDf = %10, trainDf = %80, last value = seed\n\ntestDf, cvDf, trainDf = retailDf.randomSplit([.1,.1,.8],1)\n\nprint \"trainDf count: \", trainDf.count(), \" example: \"\nfor row in trainDf.take(2): print row\nprint \"\"\n\nprint \"cvDf count: \", cvDf.count(), \" example: \"\nfor row in cvDf.take(2): print row\nprint \"\"\n\nprint \"testDf count: \", testDf.count(), \" example: \"\nfor row in testDf.take(2): print row\nprint \"\"\n", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Create ALS Object and Train Model:\n\n- import ml.recommendation (machine library) \n- proivde default hyperparameters for the ALS algorithm\n- Build recommendation models\n - Use Dataframe(trainDf) to train a model with Alternating Least Squares \n   - Latent Factors / rank<br>\n     - The number of columns in the user-feature and product-feature matricies)<br>\n   - Iterations / maxIter<br>\n     - The number of factorization runs<br>\n - Train the model(model1) - fit(trainDf) \n\n** Note:**\n- ML Pipeline: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n\n- Transformer: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. \nE.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n    \n- Estimator: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which \ntrains on a DataFrame and produces a model."}, {"execution_count": 9, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "The models has been trained\n"}], "source": "# Build the recommendation model using Alternating Least Squares algorithm (ALS) to learn the latent factors for the matrix factorization problem\n\nfrom pyspark.ml.recommendation import ALS\n\narguments = {}\narguments[\"rank\"] = 3\narguments[\"maxIter\"] = 15\narguments[\"userCol\"] = \"custId\"\narguments[\"itemCol\"] = \"stockCode\"\narguments[\"ratingCol\"] = \"purch\"\narguments[\"implicitPrefs\"] = True\n\nals1 = ALS(**arguments)\n# fit will use als1 to train the model(model1)on a dataframe (trainDf) \n# fit will train the model(model1) using the algorithm (als1) to train on a dataframe (trainDf) and produce a model. trainDf is the %80 Dataframe from the .randomsplit function\nmodel1 = als1.fit(trainDf)\n\nprint \"The models has been trained\"", "metadata": {"collapsed": false, "scrolled": true}}, {"cell_type": "markdown", "metadata": {}, "source": "\n## Prepare and Shape the Data: \nSome of the users or purchases in the cross validation data may not have been in the training data.  \nLet's remove the ones that are not, this makes it easier to test the accuracy of the model.\n\n- Import required functions\n  - import pyspark.sql.functions UserDefinedFunction\n  - import pyspark.sql.types import BooleanType\n- using sets (customers and stock) build unique collections of values for each.\n  - A set is an unordered collection with no duplicate elements\n- Print sample of data selected"}, {"execution_count": 10, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "Customer Examples\n16384\n16385\n16386\n16387\n16389\nStock Examples\n2\n90116\n21846\n90118\n90119\n"}], "source": "from pyspark.sql.functions import UserDefinedFunction\nfrom pyspark.sql.types import BooleanType\ncounter = 4\n# Sets are collections of unique elements. this will produce a distinct collection for custId(s) and stockCode(s)\n# These sets will be used to select and popuate data into the new Dataframe (cvDf) in the paragraph below. \n# the end result - all of the rows in Dataframe(cvDf) will also exist in the Dataframe (trainDf)        \n\ncustomers = set(trainDf.map(lambda line: line.custId).collect())\nprint \"Customer Examples\"\nfor i, x in enumerate(customers): \n    print x\n    if i == counter : break        \n  \nstock = set(trainDf.map(lambda line: line.stockCode).collect())\nprint \"Stock Examples\"\nfor i, x in enumerate(stock): \n    print x\n    if i == counter : break\n", "metadata": {"collapsed": false, "scrolled": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare and Shape Data:##  \n\n- Using the **_sets_** created (customers, stock) select the data that exists \n  - Print cvDf count pre .filter\n  - Create new Dataframe(cvDf)\n  - Print cvDf count post .filter "}, {"execution_count": 11, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "Pre-Filter:  25869\nPost-Filter:  25844\n"}], "source": "# filter out customers and stock codes that will be actionable (customer exists, item in stock)\nprint \"Pre-Filter: \", cvDf.count()\ncvDf = cvDf.rdd.filter(lambda line: line.stockCode in stock and\\\n                                           line.custId in customers).toDF()\nprint \"Post-Filter: \", cvDf.count()", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Test Accuracy:\n- Test the model\n  - Use the models to predict what the user will rate a certain item.  \n    - The closer our prediction of a customer purchasing a product is to 1 the better fit the model is.\n    - For example:<br>\n     - <b>Customer A</b> purchased stockCode item <b>20831</b>, if we have confidence of .9999 that the customer would purchase that product we are very accurate.\n\n- Evaluate the model with the cross validation dataframe by using the transform function.\n- Print 5 rows from the new Dataframe(predictions1) notice the additional column(prediction) created from the transformation"}, {"execution_count": 12, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "Row(custId=14286, stockCode=20831, purch=1, prediction=0.01889532431960106)\nRow(custId=13949, stockCode=20831, purch=1, prediction=0.018036123365163803)\nRow(custId=14730, stockCode=21031, purch=1, prediction=0.02368793450295925)\nRow(custId=12832, stockCode=21231, purch=1, prediction=0.03435056284070015)\nRow(custId=13038, stockCode=21231, purch=1, prediction=0.04424351453781128)\n"}], "source": "# Using ML Transform - build new Dataframe(predictions1) with additional column(predictions). \n# This will create predictions for all rows in the new dataframe. \n\npredictions1 = model1.transform(cvDf)\nfor row in predictions1.take(5):\n    print row", "metadata": {"collapsed": false, "scrolled": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare and Shape the Data: \n\n- Select specific data to use with the model\n  - Create a dataframe in which each row has the custId and an stockCode.\n- Print sample of selected data from Dataframe(userItems)"}, {"execution_count": 13, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "21231 15544\n85231 15544\n22431 15544\n23231 15544\n22631 15544\n"}], "source": "from pyspark.sql.functions import lit\n# Select using custId = 15544 and the stockCode\nstock15544 = set(trainDf.filter(trainDf['custId'] == 15544).map(lambda line: line.stockCode).collect())\n\n# Create Dataframe(userItems)\nuserItems = trainDf.select(\"stockCode\").distinct().\\\n            withColumn('custId', lit(15544)).\\\n            rdd.filter(lambda line: line.stockCode not in stock15544).toDF()\n\n# Print rows 5 times\nfor row in userItems.take(5):\n    print row.stockCode, row.custId", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Train Model:\n- Modfiy existing Dataframe(userItems) adding new column 'prediction'\n- Use 'transform' to rate the prediction of purchase for each product for this specific customer \n- Use the model(model1) to predict items the user will be interested in."}, {"execution_count": 14, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "20831 15544 0.011559555307\n21031 15544 0.0143253449351\n21231 15544 0.0551411621273\n21631 15544 0.008301098831\n22031 15544 0.0188457034528\n"}], "source": "userItems = model1.transform(userItems)\n\nfor row in userItems.take(5):\n    print row.stockCode, row.custId, row.prediction", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Train Model:\n- Print the top 5 recommendations."}, {"execution_count": 15, "cell_type": "code", "outputs": [{"data": {"text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stockCode</th>\n      <th>custId</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22382</td>\n      <td>15544</td>\n      <td>0.532846</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20725</td>\n      <td>15544</td>\n      <td>0.514003</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20726</td>\n      <td>15544</td>\n      <td>0.486621</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22629</td>\n      <td>15544</td>\n      <td>0.464256</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23209</td>\n      <td>15544</td>\n      <td>0.456238</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   stockCode  custId  prediction\n0      22382   15544    0.532846\n1      20725   15544    0.514003\n2      20726   15544    0.486621\n3      22629   15544    0.464256\n4      23209   15544    0.456238"}, "execution_count": 15, "output_type": "execute_result", "metadata": {}}], "source": "userItems.registerTempTable(\"predictions\")\nquery = \"select * from predictions order by prediction desc limit 5\"\n\nsqlContext.sql(query).toPandas()\n", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Prepare and Shape the Data:\n- Build a product Dataframe "}, {"execution_count": 16, "cell_type": "code", "outputs": [], "source": "stockItems = sqlContext.sql(\"select distinct stockCode, description from retailPurchases\")\nstockItems.registerTempTable(\"stockItems\")\n", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "## Test Accuracy:\n- Select data for specific custId (15544)\n- This user seems to have purchased a lot of childrens gifts and some holiday items.  The recomendation engine we created suggested some items along these lines\n\n**Note: The ALS algorithm uses some randomness, so the recommendations yours produces may be different than these.**"}, {"execution_count": 17, "cell_type": "code", "outputs": [{"data": {"text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stockCode</th>\n      <th>custId</th>\n      <th>prediction</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>22382</td>\n      <td>15544</td>\n      <td>0.532846</td>\n      <td>LUNCH BAG SPACEBOY DESIGN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20725</td>\n      <td>15544</td>\n      <td>0.514003</td>\n      <td>LUNCH BAG RED SPOTTY</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20725</td>\n      <td>15544</td>\n      <td>0.514003</td>\n      <td>LUNCH BAG RED RETROSPOT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20726</td>\n      <td>15544</td>\n      <td>0.486621</td>\n      <td>LUNCH BAG WOODLAND</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>22629</td>\n      <td>15544</td>\n      <td>0.464256</td>\n      <td>SPACEBOY LUNCH BOX</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>23209</td>\n      <td>15544</td>\n      <td>0.456238</td>\n      <td>LUNCH BAG DOILEY PATTERN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>23209</td>\n      <td>15544</td>\n      <td>0.456238</td>\n      <td>LUNCH BAG VINTAGE DOILEY</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>23209</td>\n      <td>15544</td>\n      <td>0.456238</td>\n      <td>LUNCH BAG VINTAGE DOILY</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>85099</td>\n      <td>15544</td>\n      <td>0.437370</td>\n      <td>JUMBO  BAG BAROQUE BLACK WHITE</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>85099</td>\n      <td>15544</td>\n      <td>0.437370</td>\n      <td>JUMBO BAG RED RETROSPOT</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   stockCode  custId  prediction                     description\n0      22382   15544    0.532846      LUNCH BAG SPACEBOY DESIGN \n1      20725   15544    0.514003            LUNCH BAG RED SPOTTY\n2      20725   15544    0.514003         LUNCH BAG RED RETROSPOT\n3      20726   15544    0.486621              LUNCH BAG WOODLAND\n4      22629   15544    0.464256             SPACEBOY LUNCH BOX \n5      23209   15544    0.456238       LUNCH BAG DOILEY PATTERN \n6      23209   15544    0.456238       LUNCH BAG VINTAGE DOILEY \n7      23209   15544    0.456238        LUNCH BAG VINTAGE DOILY \n8      85099   15544    0.437370  JUMBO  BAG BAROQUE BLACK WHITE\n9      85099   15544    0.437370         JUMBO BAG RED RETROSPOT"}, "execution_count": 17, "output_type": "execute_result", "metadata": {}}], "source": "#show recomended items\nquery = \"\"\"\nselect \n    predictions.*,\n    stockItems.description\nfrom\n    predictions\ninner join stockItems on\n    predictions.stockCode = stockItems.stockCode\norder by predictions.prediction desc\nlimit 10\n\"\"\"\nsqlContext.sql(query).toPandas()", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {"collapsed": true}, "source": "## Test Accuracy:\nNow we can refine the model and test for better accuracy\n- By changing the hyperparameters we can refine the model and test it for accuracy.<br>\n  - There are two hyperparameters we will change:<br>\n    -<b>rank</b> is the number of latent factors in the model.<br>\n    -<b>iterations</b> is the number of iterations to run.<br>"}, {"execution_count": 18, "cell_type": "code", "outputs": [], "source": "als1 = ALS(rank=3, maxIter=15,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel1 = als1.fit(trainDf)\n\nals2 = ALS(rank=15, maxIter=3,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel2 = als2.fit(trainDf)\n\nals3 = ALS(rank=15, maxIter=15,userCol=\"custId\",itemCol=\"stockCode\",ratingCol=\"purch\",implicitPrefs=True)\nmodel3 = als3.fit(trainDf)", "metadata": {"collapsed": true}}, {"cell_type": "markdown", "metadata": {}, "source": "## Test Accuracy:\nNow we can incorporate our cross validation data and determine how close of a match we have from the model"}, {"execution_count": 19, "cell_type": "code", "outputs": [], "source": "predictions1 = model1.transform(cvDf)\npredictions2 = model2.transform(cvDf)\npredictions3 = model3.transform(cvDf)", "metadata": {"collapsed": true}}, {"cell_type": "markdown", "metadata": {}, "source": "## Mean Squared Error (MSE):\n- Now we will use Mean Squared Error to determine the accuracy. This is done by comparing the prection value to the actual purchase value in our data in cvDF.\n<br>\n- Per Wikipedia: In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator measures the average of the squares of the errors or deviations, that is, the difference between the estimator and what is estimated.\n<br>\n- What we are looking for here is the lowest number as compared to the others. A perfect match, being 0, is not really possible but lower is better."}, {"execution_count": 20, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "Mean squared error = 0.7388 for our first model\nMean squared error = 0.7003 for our second model\nMean squared error = 0.6691 for our third model\n"}], "source": "meanSquaredError1 = predictions1.map(lambda line: (line.purch - line.prediction)**2).mean()\nmeanSquaredError2 = predictions2.map(lambda line: (line.purch - line.prediction)**2).mean()\nmeanSquaredError3 = predictions3.map(lambda line: (line.purch - line.prediction)**2).mean()\n    \nprint 'Mean squared error = %.4f for our first model' % meanSquaredError1\nprint 'Mean squared error = %.4f for our second model' % meanSquaredError2\nprint 'Mean squared error = %.4f for our third model' % meanSquaredError3", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "## Confirm Model:\nOnce we have determined the best model, the third model in this case. We can compare that to the testDF and again take the Mean Squared Error.\n<br>\nThe purpose of this is to make sure we are not over matched to the cvDF data. It could be that we match one subset of our data well and not another.<br>\nThe importance here is to look for a close match on the MSE value between csDF and testDF"}, {"execution_count": 21, "cell_type": "code", "outputs": [{"name": "stdout", "output_type": "stream", "text": "Mean squared error = 0.6691 for our third model using cvDF\nMean squared error = 0.6677 for our third (and best) model using testDF\n"}], "source": "filteredTestDf = testDf.rdd.filter(lambda line: line.stockCode in stock and\\\n                                              line.custId in customers).toDF()\npredictions4 = model3.transform(filteredTestDf)\nmeanSquaredError4 = predictions4.map(lambda line: (line.purch - line.prediction)**2).mean()\n\n\nprint 'Mean squared error = %.4f for our third model using cvDF' % meanSquaredError3\nprint 'Mean squared error = %.4f for our third (and best) model using testDF' % meanSquaredError4", "metadata": {"collapsed": false}}, {"cell_type": "markdown", "metadata": {}, "source": "##### Data Citation\nDaqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197\u00e2\u20ac\u201c208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17)."}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "outputs": [], "source": "", "metadata": {"collapsed": true}}], "metadata": {"kernelspec": {"display_name": "Python 2", "language": "python", "name": "python2"}, "language_info": {"codemirror_mode": {"version": 2, "name": "ipython"}, "version": "2.7.11", "name": "python", "pygments_lexer": "ipython2", "nbconvert_exporter": "python", "mimetype": "text/x-python", "file_extension": ".py"}}, "nbformat": 4}