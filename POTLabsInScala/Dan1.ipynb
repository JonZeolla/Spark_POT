{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1 - Hello Spark\n",
    "This Lab will show you how to work with Apache Spark using Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 1 - Working with Spark Context\n",
    "\n",
    "Step 1 - Invoke the spark context and extract what version of the spark driver application.\n",
    "\n",
    "Type<br>\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5.2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Step 1.1 - Check spark version\n",
    "sc.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 2 - Working with Resilient Distributed Datasets\n",
    "\n",
    "Step 2 - Create RDD with numbers 1 to 10,<br>\n",
    "Extract first line,<br>\n",
    "Extract first 5 lines,<br>\n",
    "Create RDD with string \"Hello Spark\",<br>\n",
    "Extract first line.<br>\n",
    "\n",
    "Type: <br>\n",
    "val x = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)<br>\n",
    "val x_nbr_rdd = sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "//Step 2.1 - Create RDD of Numbers 1-10\n",
    "val x = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n",
    "val x_nbr_rdd = sc.parallelize(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type: <br>\n",
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Step 2.2 - Extract first line\n",
    "x_nbr_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type:<br>\n",
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(1, 2, 3, 4, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Step 2.3 - Extract first 5 lines\n",
    "x_nbr_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type:<br>\n",
    "val y = Array(\"Hello Spark!\")<br>\n",
    "val y_str_rdd = sc.parallelize(y)<br>\n",
    "y_str_rdd.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello Spark!"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Step 2.4 - Create String RDD, Extract first line\n",
    "val y = Array(\"Hello Spark!\")\n",
    "val y_str_rdd = sc.parallelize(y)\n",
    "y_str_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 3 - Count number of lines with Spark in it\n",
    "Step 3 - Pull in a spark README.md file, <br>\n",
    "Convert the file to an RDD,<br>\n",
    "Count the number of lines with the word \"Spark\" in it. <br>\n",
    "\n",
    "Type:<br>\n",
    "import sys.process._<br>\n",
    "\"rm README.md -f\" !<br>\n",
    "\"wget https://github.com/carloapp2/SparkPOT/blob/master/README.md\" !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//Step 3.1 - Pull data file into workbench\n",
    "import sys.process._\n",
    "\"rm README.md -f\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-04-27 19:04:29--  https://github.com/carloapp2/SparkPOT/blob/master/README.md\n",
      "Resolving github.com (github.com)... 192.30.252.131\n",
      "Connecting to github.com (github.com)|192.30.252.131|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/html]\n",
      "Saving to: ‘README.md’\n",
      "\n",
      "     0K .......... .......... .......... .........              902K=0.04s\n",
      "\n",
      "2016-04-27 19:04:30 (902 KB/s) - ‘README.md’ saved [40247]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"wget https://github.com/carloapp2/SparkPOT/blob/master/README.md\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type:<br>\n",
    "val textfile_rdd = sc.textFile(\"/resources/README.md\")<br>\n",
    "textfile_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(\"\", \"\", \"\", \"\", <!DOCTYPE html>, <html lang=\"en\" class=\"\">, \"  <head prefix=\"og: http://ogp.me/ns# fb: http://ogp.me/ns/fb# object: http://ogp.me/ns/object# article: http://ogp.me/ns/article# profile: http://ogp.me/ns/profile#\">\", \"    <meta charset='utf-8'>\", \"\", \"    <link crossorigin=\"anonymous\" href=\"https://assets-cdn.github.com/assets/frameworks-777797688fb520d55f9327e3eea838b5b6ed4eab00497a0f6f0bf084319285de.css\" media=\"all\" rel=\"stylesheet\" />\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Step 3.2 - Create RDD from data file\n",
    "val textfile_rdd = sc.textFile(\"/resources/README.md\")\n",
    "textfile_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type:<br>\n",
    "val Spark_lines = textfile_rdd.filter(lines => lines.contains(\"Spark\"))<br>\n",
    "Spark_lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"    <title>SparkPOT/README.md at master · carloapp2/SparkPOT · GitHub</title>\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Step 3.3 - Filter for only lines with word Spark\n",
    "val Spark_lines = textfile_rdd.filter(lines => lines.contains(\"Spark\"))\n",
    "Spark_lines.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type:<br>\n",
    "println(\"The file README.md has \" + Spark_lines.count() + \" of \" + textfile_rdd.count() + \" Lines with word Spark in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file README.md has 49 of 595 Lines with word Spark in it.\n"
     ]
    }
   ],
   "source": [
    "//Step 3.4 - count the number of lines\n",
    "println(\"The file README.md has \" + Spark_lines.count() + \" of \" + textfile_rdd.count() + \" Lines with word Spark in it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Step 4 - Perform analysis on a data file\n",
    "We have a sample file with instructors and scores. This exercise we want you to add all scores and report on results.\n",
    "\n",
    "Load File Instructor-Scores,<br>\n",
    "Map name and scores into RDD,<br>\n",
    "Add the 4 numbers per instructor,<br>\n",
    "Print the total score for each instructor<br>\n",
    "Print average score for each instructor<br>\n",
    "Who was top performer?<br>\n",
    "\n",
    "Data File Format: Name,Score1,Score2,Score3,Score4<br>\n",
    "Example Line: \"Carlo,5,3,3,4\"<br>\n",
    "Data File Location: https://github.com/carloapp2/SparkPOT/blob/master/Instructor-Scores.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "//Step 4\n",
    "\"rm Instructor-Scores.txt -f\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-04-18 04:59:27--  https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Instructor-Scores.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.27.76.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.27.76.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84 [text/plain]\n",
      "Saving to: ‘Instructor-Scores.txt’\n",
      "\n",
      "     0K                                                       100% 4.05M=0s\n",
      "\n",
      "2016-04-18 04:59:27 (4.05 MB/s) - ‘Instructor-Scores.txt’ saved [84/84]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Instructor-Scores.txt\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Carlo,15,3)\n",
      "(\"Mokhtar,15,3)\n",
      "(\"Jacques,15,3)\n",
      "(\"Braden,15,3)\n",
      "(\"Chris,15,3)\n"
     ]
    }
   ],
   "source": [
    "val textfile_rdd = sc.textFile(\"/resources/Instructor-Scores.txt\")\n",
    "val totalScore = textfile_rdd.map(line=>line.split(',')).map(vals=>(vals(0),(vals(1).toInt+vals(2).toInt+vals(3).toInt+vals(4).substring(0,1).toInt),((vals(1).toInt+vals(2).toInt+vals(3).toInt+vals(4).substring(0,1).toInt)/4)))\n",
    "totalScore.take(5).foreach(println)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.10.4 (Spark 1.5.2)",
   "language": "scala",
   "name": "spark"
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
